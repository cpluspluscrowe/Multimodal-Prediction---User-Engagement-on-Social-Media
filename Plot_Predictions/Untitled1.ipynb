{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from Notebooks.Text.Process import to_vector\n",
    "import pickle\n",
    "from PIL import ImageFile\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from Notebooks.LinkDatabases.FacebookData import FacebookDataDatabase\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import load_model\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from Notebooks.Plot.plot_training import PlotLearning\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class Static:\n",
    "    facebookDb = FacebookDataDatabase()\n",
    "    metric_getter = facebookDb.getSentiment  # Set this to change the model type\n",
    "    group_size = 30000\n",
    "    limit = 35000\n",
    "    plot_losses = PlotLearning(\"combined_keras_model_sentiment\")\n",
    "    batch_size = 1\n",
    "    epochs = 20\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    pkl_filename = Static.metric_getter.__name__ + \".pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model_name = Static.metric_getter.__name__ + \".pkl\"\n",
    "    with open(model_name, 'rb') as pickle_file:\n",
    "        return pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "def group(iterator, count):\n",
    "    itr = iter(iterator)\n",
    "    while True:\n",
    "        yield tuple([next(itr) for i in range(count)])\n",
    "\n",
    "\n",
    "def get_size(filename):\n",
    "    st = os.stat(filename)\n",
    "    return st.st_size\n",
    "\n",
    "\n",
    "def to_array(images):\n",
    "    return np.array(list(map(lambda x: img_to_array(x) / 255, images)))\n",
    "\n",
    "\n",
    "def load_images(files):\n",
    "    loaded = []\n",
    "    for file_to_load in files:\n",
    "        try:\n",
    "            load = image.load_img(file_to_load, target_size=(64, 64))\n",
    "            loaded.append(load)\n",
    "        except:\n",
    "            print(\"Could not load file: \", file_to_load, os.path.exists(file_to_load))\n",
    "    return loaded\n",
    "\n",
    "\n",
    "def transform_image_data(all_files):\n",
    "    files = list(filter(lambda x: \".jpg\" in x or \".png\" in x, filter(lambda x: \".DS_Store\" not in x, all_files)))\n",
    "    image_paths = list(map(lambda x: os.path.join(\"../Image_CNN/images\", x), files))\n",
    "    image_paths = list(filter(lambda x: get_size(x) > 1, image_paths))\n",
    "    image_arrays = to_array(load_images(image_paths))\n",
    "    return image_arrays\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    sentiment_post_ids = list(map(lambda x: x[0], Static.facebookDb.getImageIdWithPositiveCommentCounts()))\n",
    "    all_dir_files = os.listdir(\"../Image_CNN/images\")\n",
    "\n",
    "    all_files = []\n",
    "    for post_id in sentiment_post_ids:\n",
    "        for file_name in all_dir_files:\n",
    "            if post_id in file_name:\n",
    "                all_files.append(file_name)\n",
    "    print(\"Number of files in analysis: {0}\".format(len(all_files)))\n",
    "    print(all_files)\n",
    "    ids = list(map(lambda x: x[:-4], all_files))\n",
    "    rows = list(\n",
    "        map(lambda x: x[0], filter(lambda x: x if x else None, map(lambda x: Static.facebookDb.getRow(x), ids))))\n",
    "    data = list(map(lambda x: (x[0], x[10], x[2], x[3]), rows))\n",
    "    messages = list(map(lambda x: x[1], data))\n",
    "    image_data = transform_image_data(all_files)\n",
    "    message_data = to_vector(messages)\n",
    "    y_data = list(map(lambda x: x if x > 0 else 0,\n",
    "                      filter(lambda x: x != None, map(lambda x: Static.metric_getter(x), ids))))\n",
    "    combined_data = zip(image_data, message_data, y_data)\n",
    "    if Static.group_size is None:\n",
    "        Static.group_size = len(list(combined_data)) - 10000\n",
    "    import statistics\n",
    "    print(\"Data Var: \", statistics.stdev(y_data) ** 2)\n",
    "    for data_chunk in group(combined_data, Static.group_size):  # ):group(\n",
    "        image_data_batch, message_data_batch, y_data_batch = zip(*data_chunk)\n",
    "        (trainX_image, testX_image, trainY_image, testY_image) = train_test_split(image_data_batch, y_data_batch,\n",
    "                                                                                  test_size=0.25, random_state=42)\n",
    "        (trainX_message, testX_message, trainY_message, testY_message) = train_test_split(message_data_batch,\n",
    "                                                                                          y_data_batch, test_size=0.25,\n",
    "                                                                                          random_state=42)\n",
    "        assert trainY_image == trainY_message\n",
    "        assert testY_image == testY_message\n",
    "        yield trainX_image, trainX_message, trainY_image, testX_image, testX_message, testY_image\n",
    "\n",
    "\n",
    "def find_models(model_name):\n",
    "    model_paths = []\n",
    "    for file in os.listdir(\"./\"):\n",
    "        if model_name in file and \".h5\" in file:\n",
    "            model_paths.append(os.path.join(\"./\", file))\n",
    "    return model_paths\n",
    "\n",
    "\n",
    "def get_models():\n",
    "    loaded_model = []  # print(get_models())\n",
    "    for model_path in find_models(Static.metric_getter.__name__):\n",
    "        model = load_model(model_path)\n",
    "        loaded_model.append(model)\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def get_model_by_name(name):\n",
    "    model = load_model(find_models(name)[0])\n",
    "    return model\n",
    "\n",
    "\n",
    "models = get_models()\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def get_predictions(image_data, message_data, answers, image_model, message_model):\n",
    "    message_data_squeezed = np.squeeze(np.array(message_data), axis=1)\n",
    "    image_predictions = image_model.predict(np.array(image_data)).tolist()\n",
    "    message_predictions = message_model.predict(message_data_squeezed).tolist()\n",
    "    predictions = [[i[0], j[0]] for i, j in zip(image_predictions, message_predictions)]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def create_nlp_model():\n",
    "    model = Sequential()\n",
    "    input_dim = 10000\n",
    "    model.add(Dense(64, input_dim=input_dim))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(units=32, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_dim=input_dim))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Dense(units=8, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_dim=input_dim))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnn_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, input_shape=(64, 64, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "nlp_model = create_nlp_model()\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "mergedOut = keras.layers.Add()([cnn_model.output, nlp_model.output])\n",
    "mergedOut = Dense(8, activation='relu')(mergedOut)\n",
    "mergedOut = Dropout(.35)(mergedOut)\n",
    "mergedOut = Dense(1, activation='relu')(mergedOut)\n",
    "newModel = Model([cnn_model.input, nlp_model.input], outputs=mergedOut)\n",
    "newModel.compile(optimizer=Adam(lr=1e-04), loss='mean_squared_error',\n",
    "                 metrics=['mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error',\n",
    "                          'cosine_proximity', 'accuracy'])\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model_path = \"Combined_Model_{0}.h5\".format(Static.metric_getter.__name__)\n",
    "if os.path.exists(model_path):\n",
    "    # overwrites other newModel\n",
    "    # newModel = load_model(model_path)\n",
    "    print(\"loading existing model\")\n",
    "\n",
    "for trainX_image, trainX_message, trainY, testX_image, testX_message, testY in get_data():\n",
    "    trainY = list(map(lambda x: x * 100, trainY))\n",
    "    testY = list(map(lambda x: x * 100, testY))\n",
    "\n",
    "    bins = numpy.linspace(-100, 100, 100)\n",
    "\n",
    "    pyplot.hist(trainY, bins, alpha=0.5, label='Comment Sentiment',)\n",
    "    pyplot.title(\"Comment Sentiment Histogram\")\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.xlabel(\"Comment Sentiment\")\n",
    "    pyplot.ylabel(\"Bin Count\")\n",
    "    pyplot.savefig('getCommentSentiment_prediction_vs_test_hist.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
