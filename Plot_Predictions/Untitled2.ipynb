{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "%matplotlib inline\n",
    "import random\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from Notebooks.Text.Process import to_vector\n",
    "import pickle\n",
    "from PIL import ImageFile\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from Notebooks.LinkDatabases.FacebookData import FacebookDataDatabase\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.models import load_model\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from Notebooks.Plot.plot_training import PlotLearning\n",
    "from keras.models import load_model\n",
    "print(os.getcwd())\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class Static:\n",
    "    facebookDb = FacebookDataDatabase()\n",
    "    metric_getter = facebookDb.getShareCount  # Set this to change the model type\n",
    "    group_size = 5000\n",
    "    limit = 10000\n",
    "    plot_losses = PlotLearning(\"combined_keras_model\")\n",
    "    batch_size = 128\n",
    "    epochs = 20\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    pkl_filename = Static.metric_getter.__name__ + \".pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model_name = Static.metric_getter.__name__ + \".pkl\"\n",
    "    with open(model_name, 'rb') as pickle_file:\n",
    "        return pickle.load(pickle_file)\n",
    "\n",
    "\n",
    "def group(iterator, count):\n",
    "    itr = iter(iterator)\n",
    "    while True:\n",
    "        yield tuple([next(itr) for i in range(count)])\n",
    "\n",
    "\n",
    "def get_size(filename):\n",
    "    st = os.stat(filename)\n",
    "    return st.st_size\n",
    "\n",
    "\n",
    "def to_array(images):\n",
    "    return np.array(list(map(lambda x: img_to_array(x) / 255, images)))\n",
    "\n",
    "\n",
    "def load_images(files):\n",
    "    exists = list(filter(lambda x: os.path.exists(x),files))\n",
    "    return list(map(lambda x: image.load_img(x, target_size=(64, 64)), exists))\n",
    "\n",
    "\n",
    "def transform_image_data(all_files):\n",
    "    files = list(filter(lambda x: \".jpg\" in x or \".png\" in x, filter(lambda x: \".DS_Store\" not in x, all_files)))\n",
    "    image_paths = list(map(lambda x: os.path.join(\"../Image_CNN/images\", x), files))\n",
    "    image_paths = list(filter(lambda x: get_size(x) > 1, image_paths))\n",
    "    image_arrays = to_array(load_images(image_paths))\n",
    "    return image_arrays\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    all_files = os.listdir(\"../Image_CNN/images\")[:Static.limit:]\n",
    "    ids = list(map(lambda x: x[:-4], all_files))\n",
    "    # for id in ids:\n",
    "    # value = Static.facebookDb.getRow(id)\n",
    "    # print(id, \": \", value)\n",
    "    rows = list(\n",
    "        map(lambda x: x[0], filter(lambda x: x if x else None, map(lambda x: Static.facebookDb.getRow(x), ids))))\n",
    "    data = list(map(lambda x: (x[0], x[10], x[2], x[3]), rows))\n",
    "    messages = list(map(lambda x: x[1], data))\n",
    "    # share_counts = list(map(lambda x: x[2], data))\n",
    "    comment_counts = list(map(lambda x: x[3], data))\n",
    "    image_data = transform_image_data(all_files)\n",
    "    message_data = to_vector(messages)\n",
    "    y_data = list(map(lambda x: x if x > 0 else 0, map(lambda x: Static.metric_getter(x), ids)))\n",
    "    combined_data = zip(image_data, message_data, y_data)\n",
    "    import statistics\n",
    "    print(\"Data Var: \", statistics.stdev(y_data) ** 2)\n",
    "    for data_chunk in group(combined_data, Static.group_size):  # ):group(\n",
    "        image_data_batch, message_data_batch, y_data_batch = zip(*data_chunk)\n",
    "        (trainX_image, testX_image, trainY_image, testY_image) = train_test_split(image_data_batch, y_data_batch,\n",
    "                                                                                  test_size=0.25, random_state=42)\n",
    "        (trainX_message, testX_message, trainY_message, testY_message) = train_test_split(message_data_batch,\n",
    "                                                                                          y_data_batch, test_size=0.25,\n",
    "                                                                                          random_state=42)\n",
    "        assert trainY_image == trainY_message\n",
    "        assert testY_image == testY_message\n",
    "        yield trainX_image, trainX_message, trainY_image, testX_image, testX_message, testY_image\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def get_predictions(image_data, message_data, answers, image_model, message_model):\n",
    "    message_data_squeezed = np.squeeze(np.array(message_data), axis=1)\n",
    "    image_predictions = image_model.predict(np.array(image_data)).tolist()\n",
    "    message_predictions = message_model.predict(message_data_squeezed).tolist()\n",
    "    predictions = [[i[0], j[0]] for i, j in zip(image_predictions, message_predictions)]\n",
    "    return predictions\n",
    "\n",
    "for trainX_image, trainX_message, trainY, testX_image, testX_message, testY in get_data():\n",
    "    #model = load_model(\"../NLP_NN/nlp_getShareCount.h5\")\n",
    "    #predictions = model.predict(np.squeeze(np.array(trainX_message), axis=1))#model.predict([np.array(trainX_image), np.squeeze(np.array(trainX_message), axis=1)])\n",
    "    #flatten_predictions = list(map(lambda x: x[0], predictions))\n",
    "\n",
    "    bins = numpy.linspace(0, 100, 100)\n",
    "\n",
    "    pyplot.hist(trainY, bins, alpha=0.5, label='Share Counts',)\n",
    "    pyplot.title(\"Share Count Histogram\")\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.xlabel(\"Count\")\n",
    "    pyplot.ylabel(\"Bin Count\")\n",
    "    pyplot.savefig('getShareCount_prediction_vs_test_hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
